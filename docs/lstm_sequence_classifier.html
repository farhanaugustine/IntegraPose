<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IntegraPose Real-Time LSTM Classification Guide</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* slate-50 */
        }
        .section-card {
            background-color: white;
            border-radius: 0.75rem;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
        }
        .step {
            display: flex;
            align-items: flex-start;
            margin-bottom: 2.5rem;
        }
        .step-number {
            flex-shrink: 0;
            background-color: #0284c7; /* sky-600 */
            color: white;
            width: 2.5rem;
            height: 2.5rem;
            border-radius: 9999px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 1.25rem;
            margin-right: 1.5rem;
            margin-top: 0.25rem;
        }
        .admonition {
            border-left-width: 4px;
            padding: 1rem;
            border-radius: 0.25rem;
            margin-top: 1rem;
            margin-bottom: 1rem;
        }
        .admonition-info {
            border-color: #3b82f6; /* blue-500 */
            background-color: #eff6ff; /* blue-50 */
            color: #1e40af; /* blue-800 */
        }
        .admonition-success {
            border-color: #22c55e; /* green-500 */
            background-color: #f0fdf4; /* green-50 */
            color: #166534; /* green-800 */
        }
        kbd {
            font-family: monospace;
            background-color: #e5e7eb; /* gray-200 */
            color: #1f2937; /* gray-800 */
            padding: 0.1rem 0.4rem;
            border-radius: 0.25rem;
            font-size: 0.875em;
            border: 1px solid #d1d5db; /* gray-300 */
        }
        .code-block {
             background-color: #1f2937; /* gray-800 */
             color: #d1d5db; /* gray-300 */
             padding: 1rem;
             border-radius: 0.5rem;
             font-family: monospace;
             font-size: 0.875rem;
             overflow-x: auto;
             white-space: pre;
        }
        .workflow-arrow {
            text-align: center;
            font-size: 2rem;
            color: #94a3b8; /* slate-400 */
            margin: 1rem 0;
        }
        .directory-structure {
            background-color: #f1f5f9; /* slate-100 */
            color: #475569; /* slate-600 */
            padding: 1rem;
            border-radius: 0.5rem;
            font-family: monospace;
            font-size: 0.875rem;
            overflow-x: auto;
            border: 1px solid #e2e8f0; /* slate-200 */
        }
    </style>
</head>
<body class="text-slate-800">

    <header class="bg-white/80 backdrop-blur-lg sticky top-0 z-50 border-b border-slate-200">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between items-center h-16">
                <div class="flex-shrink-0">
                    <h1 class="text-xl font-bold text-slate-900">IntegraPose User Guide</h1>
                </div>
            </div>
        </div>
    </header>

    <main class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-12">

        <section id="intro" class="mb-16">
            <div class="text-center">
                <h2 class="text-3xl font-bold tracking-tight text-slate-900 sm:text-4xl">Supervised, Real-Time LSTM Classification</h2>
                <p class="mt-4 text-lg leading-8 text-slate-600">
                    This guide details a powerful, automated workflow for training a temporal deep learning model (LSTM) to classify animal behaviors in real-time. This toolkit leverages custom-trained pose-estimation models to create a robust, supervised classification pipeline, controlled by a single command-line interface.
                </p>
            </div>
        </section>

        <section id="structure" class="mb-16">
             <div class="section-card p-6 md:p-8">
                <h3 class="text-2xl font-semibold mb-4 text-slate-900">🚀 Project Structure</h3>
                <p class="text-slate-600 mb-4">
                    The toolkit is organized into a modular structure for better maintainability and clarity. All operations are run from the root of the <kbd>integrapose_lstm/</kbd> directory.
                </p>
                <div class="directory-structure my-4">
<pre>📁 integrapose_lstm/
    ├─ 📁 data_processing/
    │  ├─ features.py
    │  ├─ loader.py
    │  ├─ preparation.py
    │  └─ process_clips.py
    ├─ 📁 gui/
    │  └─ clipper.py
    ├─ 📁 inference/
    │  └─ real_time.py
    ├─ 📁 training/
    │  ├─ models.py
    │  └─ train.py
    ├─ 📜 main.py
    ├─ 📜 run_training.py
    ├─ 📜 config.json
    └─ 📜 requirements.txt</pre>
                </div>
             </div>
        </section>

        <section id="workflow" class="mb-16">
             <div class="section-card p-6 md:p-8">
                <h3 class="text-2xl font-semibold mb-6 text-slate-900 text-center">The Complete End-to-End Workflow</h3>
                
                <div class="p-4 border rounded-lg bg-slate-50">
                    <h4 class="font-bold text-lg text-slate-800">Phase 1: Pose Model Training (IntegraPose Main Toolkit)</h4>
                    <ul class="list-disc list-inside mt-2 text-slate-700">
                        <li><strong>Input:</strong> Raw video frames for keypoint labeling.</li>
                        <li><strong>Action:</strong> Use the main IntegraPose GUI to label keypoints and train a custom YOLO-pose model.</li>
                        <li><strong>Output:</strong> A trained pose-estimation model (e.g., <kbd>yolo_pose_model.pt</kbd>).</li>
                    </ul>
                </div>
                
                <div class="workflow-arrow">⬇️</div>

                <div class="p-4 border rounded-lg bg-slate-50">
                    <h4 class="font-bold text-lg text-slate-800">Phase 2: Labeled Clip & Pose Data Generation (This Toolkit)</h4>
                    <ul class="list-disc list-inside mt-2 text-slate-700">
                        <li><strong>Input:</strong> A long, unlabeled video and your trained YOLO model.</li>
                        <li><strong>Action 1 (Clipping):</strong> Run <kbd>python main.py clip</kbd> to create and save short video clips of specific behaviors.</li>
                        <li><strong>Action 2 (Automated Pose Estimation):</strong> Run <kbd>python main.py process</kbd> to automatically generate <kbd>.txt</kbd> pose data for all clips.</li>
                        <li><strong>Action 3 (Consolidation):</strong> Run <kbd>python main.py prepare</kbd> to create the final, unified training dataset.</li>
                        <li><strong>Output:</strong> A folder of labeled pose data ready for training.</li>
                    </ul>
                </div>

                <div class="workflow-arrow">⬇️</div>

                <div class="p-4 border rounded-lg bg-slate-50">
                    <h4 class="font-bold text-lg text-slate-800">Phase 3: LSTM Classifier Training (This Toolkit)</h4>
                    <ul class="list-disc list-inside mt-2 text-slate-700">
                        <li><strong>Input:</strong> The labeled pose dataset from Phase 2.</li>
                        <li><strong>Action:</strong> Run <kbd>python main.py train</kbd>.</li>
                        <li><strong>Output:</strong> A trained LSTM classifier (<kbd>lstm_classifier.pt</kbd>) and normalization statistics (<kbd>norm_stats.pkl</kbd>).</li>
                    </ul>
                </div>

                <div class="workflow-arrow">⬇️</div>
                
                <div class="p-4 border rounded-lg bg-slate-50">
                    <h4 class="font-bold text-lg text-slate-800">Phase 4: Real-Time Deployment (This Toolkit)</h4>
                    <ul class="list-disc list-inside mt-2 text-slate-700">
                        <li><strong>Input:</strong> Both the <kbd>yolo_pose_model.pt</kbd> and the <kbd>lstm_classifier.pt</kbd>.</li>
                        <li><strong>Action:</strong> Run <kbd>python main.py infer</kbd> on a new video.</li>
                        <li><strong>Output:</strong> Live, frame-by-frame behavioral classification.</li>
                    </ul>
                </div>
             </div>
        </section>
        
        <section id="tutorial-steps">
            <h2 class="text-3xl font-bold tracking-tight text-slate-900 sm:text-4xl text-center mb-12">Step-by-Step Guide</h2>
            <div class="section-card p-6 md:p-8">

                <div class="admonition admonition-info">
                    <p class="font-bold">Prerequisites</p>
                    <ul class="list-disc list-inside mt-2">
                        <li>You must have a custom-trained YOLO-pose model from the main IntegraPose toolkit.</li>
                        <li>Install all required packages by running <kbd>pip install -r requirements.txt</kbd> in your terminal.</li>
                    </ul>
                </div>

                <div class="step">
                    <div class="step-number">1</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 1: Create Labeled Video Clips</h4>
                        <p class="text-slate-600 mb-4">Generate a dataset of short video clips, each containing a single, clear example of a behavior, using the Video Clipper GUI.</p>
                        
                        <h5 class="font-semibold text-lg mt-4 mb-2">Configuration (`config.json`)</h5>
                        <p class="text-slate-600 mb-2">First, define your behaviors and keyboard shortcuts in the <kbd>dataset_preparation</kbd> section of your config file.</p>
                        <div class="code-block mt-2">
"dataset_preparation": {
  "source_directory": "clips_for_labeling",
  "key_mapping": { "w": "Walking", "r": "Rearing", ... },
  "class_mapping": { "Walking": 0, "Rearing": 2, ... }
}
                        </div>

                        <h5 class="font-semibold text-lg mt-4 mb-2">Running the GUI & Labeling</h5>
                        <div class="code-block mt-2">python main.py clip</div>
                        <ol class="list-decimal list-inside space-y-2 text-slate-700 mt-4">
                             <li>Click <kbd>Load Video</kbd> to open a long video you want to label.</li>
                             <li>Find a behavior, then use <kbd>Set Start Frame</kbd> and <kbd>Set End Frame</kbd> to mark its boundaries.</li>
                             <li>Assign a label with a keypress (e.g., <kbd>w</kbd> for "Walking").</li>
                             <li>Click <kbd>Add Clip to Queue</kbd>. You can queue many clips for multiple behaviors.</li>
                             <li>When finished, click <kbd>Extract All Queued Clips</kbd>. The GUI will save the clips into a directory structure like the one below.</li>
                        </ol>
                        <div class="directory-structure my-4">
<pre>📁 clips_for_labeling/
    ├─ 📁 Walking/
    │  └─ your_video_Walking_frames_100_250.mp4
    └─ 📁 Rearing/
       └─ your_video_Rearing_frames_550_700.mp4</pre>
                        </div>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">2</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 2: Automate Pose Data Generation</h4>
                        <p class="text-slate-600 mb-4">This command automates the previously manual process of generating keypoint data for all your video clips.</p>

                        <h5 class="font-semibold text-lg mt-4 mb-2">Configuration (`config.json`)</h5>
                        <p class="text-slate-600 mb-2">Ensure the path to your trained YOLO model is correct in the <kbd>inference_params</kbd> section.</p>
                        <div class="code-block mt-2">
"inference_params": {
  "yolo_model_path": "path/to/your/yolo_pose_model.pt",
  ...
}
                        </div>

                        <h5 class="font-semibold text-lg mt-4 mb-2">Execution</h5>
                        <div class="code-block mt-2">python main.py process</div>
                        <p class="text-slate-600 mt-2">The script will scan all subfolders in your <kbd>source_directory</kbd>, run your YOLO model on every <kbd>.mp4</kbd> file, and save the resulting <kbd>.txt</kbd> pose files directly into the correct behavior folder.</p>
                        
                        <div class="admonition admonition-success mt-4">
                            <p class="font-bold">Automation Complete!</p>
                            <p>The tedious manual step of running YOLO and moving files is now handled by a single command. Each behavior folder now contains both video clips and their corresponding pose data.</p>
                        </div>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">3</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 3: Prepare the Final Dataset</h4>
                        <p class="text-slate-600 mb-4">This script automates the final consolidation of your pose data into a training-ready format.</p>
                        
                        <h5 class="font-semibold text-lg mt-4 mb-2">Configuration and Execution</h5>
                        <p class="text-slate-600 mb-2">Ensure your <kbd>data.pose_directory</kbd> in <kbd>config.json</kbd> is set to where you want the final dataset to be stored. Then run:</p>
                        <div class="code-block mt-2">python main.py prepare</div>
                        <p class="text-slate-600 mt-2">The script scans the subfolders, finds all <kbd>.txt</kbd> files, replaces their internal class ID with the correct one from your config, and copies them into the final training directory.</p>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">4</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 4: Train the LSTM Classifier</h4>
                        <p class="text-slate-600 mb-4">This stage trains the LSTM model on your prepared dataset.</p>
                        
                        <h5 class="font-semibold text-lg mt-4 mb-2">Understanding the Sequence Length</h5>
                        <p class="text-slate-600 mb-2">A critical parameter is the <kbd>sequence_length</kbd> in your config, which defines the LSTM's "rolling buffer" or "memory".</p>
                        <div class="code-block mt-2">
"sequence_params": {
  "sequence_length": 15
}
                        </div>
                        <p class="text-slate-600 mt-2">This value dictates how many consecutive frames the LSTM analyzes to make a single prediction. A shorter length (e.g., 15 frames at 30fps = 0.5s) is good for quick behaviors, while a longer length may better capture complex sequences but will have a greater classification delay.</p>

                        <h5 class="font-semibold text-lg mt-4 mb-2">Running the Training</h5>
                        <div class="code-block mt-2">python main.py train</div>
                        <p class="text-slate-600 mt-2">The script will compute features, create sequences, and train the model. It saves the best model (<kbd>lstm_classifier.pt</kbd>) and normalization data (<kbd>norm_stats.pkl</kbd>) to your output folder.</p>
                    </div>
                </div>

                 <div class="step">
                    <div class="step-number">5</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 5: Run Real-Time Inference</h4>
                        <p class="text-slate-600 mb-4">Deploy your models for live classification.</p>
                        
                        <h5 class="font-semibold text-lg mt-4 mb-2">Configuration</h5>
                        <p class="text-slate-600 mb-2">Update the <kbd>video_source</kbd> in your <kbd>config.json</kbd> to point to the video you wish to analyze.</p>
                        <div class="code-block mt-2">
"inference_params": {
  "yolo_model_path": "...",
  "video_source": "path/to/your/inference_video.mp4"
}
                        </div>

                        <h5 class="font-semibold text-lg mt-4 mb-2">Execution</h5>
                        <div class="code-block mt-2">python main.py infer</div>
                        <p class="text-slate-600 mt-2">A window will appear showing the live classification. Press <kbd>q</kbd> to quit the video stream.</p>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <footer class="border-t border-slate-200 mt-12 py-8">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center text-slate-500">
            <p>IntegraPose Real-Time LSTM Classification Guide</p>
        </div>
    </footer>

</body>
</html>
