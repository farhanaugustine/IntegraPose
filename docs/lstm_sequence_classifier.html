<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IntegraPose Real-Time LSTM Classification Guide</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* slate-50 */
        }
        .section-card {
            background-color: white;
            border-radius: 0.75rem;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
        }
        .step {
            display: flex;
            align-items: flex-start;
            margin-bottom: 2.5rem;
        }
        .step-number {
            flex-shrink: 0;
            background-color: #0284c7; /* sky-600 */
            color: white;
            width: 2.5rem;
            height: 2.5rem;
            border-radius: 9999px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 1.25rem;
            margin-right: 1.5rem;
            margin-top: 0.25rem;
        }
        .admonition {
            border-left-width: 4px;
            padding: 1rem;
            border-radius: 0.25rem;
            margin-top: 1rem;
            margin-bottom: 1rem;
        }
        .admonition-info {
            border-color: #3b82f6; /* blue-500 */
            background-color: #eff6ff; /* blue-50 */
            color: #1e40af; /* blue-800 */
        }
        .admonition-warning {
            border-color: #f59e0b; /* amber-500 */
            background-color: #fffbeb; /* amber-50 */
            color: #b45309; /* amber-800 */
        }
        kbd {
            font-family: monospace;
            background-color: #e5e7eb; /* gray-200 */
            color: #1f2937; /* gray-800 */
            padding: 0.1rem 0.4rem;
            border-radius: 0.25rem;
            font-size: 0.875em;
            border: 1px solid #d1d5db; /* gray-300 */
        }
        .code-block {
             background-color: #1f2937; /* gray-800 */
             color: #d1d5db; /* gray-300 */
             padding: 1rem;
             border-radius: 0.5rem;
             font-family: monospace;
             font-size: 0.875rem;
             overflow-x: auto;
             white-space: pre;
        }
        .workflow-arrow {
            text-align: center;
            font-size: 2rem;
            color: #94a3b8; /* slate-400 */
            margin: 1rem 0;
        }
        .directory-structure {
            background-color: #f1f5f9; /* slate-100 */
            color: #475569; /* slate-600 */
            padding: 1rem;
            border-radius: 0.5rem;
            font-family: monospace;
            font-size: 0.875rem;
            overflow-x: auto;
            border: 1px solid #e2e8f0; /* slate-200 */
        }
    </style>
</head>
<body class="text-slate-800">

    <header class="bg-white/80 backdrop-blur-lg sticky top-0 z-50 border-b border-slate-200">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between items-center h-16">
                <div class="flex-shrink-0">
                    <h1 class="text-xl font-bold text-slate-900">IntegraPose User Guide</h1>
                </div>
            </div>
        </div>
    </header>

    <main class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-12">

        <section id="intro" class="mb-16">
            <div class="text-center">
                <h2 class="text-3xl font-bold tracking-tight text-slate-900 sm:text-4xl">Supervised, Real-Time LSTM Classification</h2>
                <p class="mt-4 text-lg leading-8 text-slate-600">
                    This guide details a powerful workflow for training a temporal deep learning model (LSTM) to classify animal behaviors in real-time. This approach complements the main IntegraPose toolkit by leveraging its custom-trained pose-estimation models to create a robust, supervised classification pipeline.
                </p>
            </div>
        </section>

        <section id="philosophy" class="mb-16">
             <div class="section-card p-6 md:p-8">
                <h3 class="text-2xl font-semibold mb-4 text-slate-900">üß† Conceptual Framework: A Tale of Two Models</h3>
                <p class="text-slate-600 mb-4">
                    This workflow achieves its accuracy by strategically dividing the analytical labor between two specialized models: a YOLO model for spatial awareness and an LSTM for temporal understanding.
                </p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div class="p-4 rounded-lg bg-slate-50 border border-slate-200">
                        <h4 class="font-semibold text-lg text-slate-800">1. The YOLO Model: The "What" and "Where"</h4>
                        <p class="text-slate-600 mt-2">
                            Within this pipeline, the IntegraPose YOLO model serves as the frontline visual processor. Its responsibilities are strictly confined to the present moment:
                        </p>
                        <ul class="list-disc list-inside mt-2 space-y-2 text-slate-700">
                            <li><strong>Pose Estimator:</strong> For each video frame, it answers the question, "Where are the subject's body parts?" by providing precise (x, y) coordinates for each keypoint.</li>
                            <li><strong>Object Classifier:</strong> If trained for it, it can provide a basic, static classification for that single frame (e.g., this pose looks like "rearing").</li>
                        </ul>
                        <p class="text-slate-600 mt-2">
                           Crucially, the YOLO model is stateless; it has no memory of past frames. It analyzes each frame as an independent photograph, lacking the context to understand the dynamics of movement.
                        </p>
                    </div>
                    <div class="p-4 rounded-lg bg-slate-50 border border-slate-200">
                        <h4 class="font-semibold text-lg text-slate-800">2. The LSTM Classifier: The "How" and "Why"</h4>
                        <p class="text-slate-600 mt-2">
                            The burden of true behavioral classification is offloaded to the Long Short-Term Memory (LSTM) model. The LSTM's architecture is fundamentally designed to understand sequences and temporal dependencies.
                        </p>
                         <ul class="list-disc list-inside mt-2 space-y-2 text-slate-700">
                             <li><strong>Temporal Context:</strong> It takes a sequence of poses from YOLO and answers the question, "Given the movements over the last N frames, what behavior is occurring?".</li>
                             <li><strong>Dynamic Patterns:</strong> It learns the unique temporal signatures of behaviors. For example, it learns that "grooming" involves a specific oscillating pattern of the front paws near the head, a pattern invisible to a single-frame analysis.</li>
                         </ul>
                        <p class="text-slate-600 mt-2">
                           By focusing on sequences, the LSTM can differentiate between visually similar but dynamically distinct actions, providing a much more robust and accurate classification of behavior.
                        </p>
                    </div>
                </div>
             </div>
        </section>

        <section id="workflow" class="mb-16">
             <div class="section-card p-6 md:p-8">
                <h3 class="text-2xl font-semibold mb-6 text-slate-900 text-center">The Complete End-to-End Workflow</h3>
                
                <div class="p-4 border rounded-lg bg-slate-50">
                    <h4 class="font-bold text-lg text-slate-800">Phase 1: Pose Model Training (IntegraPose Main Toolkit)</h4>
                    <ul class="list-disc list-inside mt-2 text-slate-700">
                        <li><strong>Input:</strong> Raw video frames for keypoint labeling.</li>
                        <li><strong>Action:</strong> Use the main IntegraPose GUI to label keypoints and train a custom YOLO-pose model.</li>
                        <li><strong>Output:</strong> A trained pose-estimation model (e.g., <kbd>yolo_pose_model.pt</kbd>).</li>
                    </ul>
                </div>
                
                <div class="workflow-arrow">‚¨áÔ∏è</div>

                <div class="p-4 border rounded-lg bg-slate-50">
                    <h4 class="font-bold text-lg text-slate-800">Phase 2: Labeled Clip & Pose Data Generation (This Project)</h4>
                    <ul class="list-disc list-inside mt-2 text-slate-700">
                        <li><strong>Input:</strong> A long, unlabeled video.</li>
                        <li><strong>Action 1 (Clipping):</strong> Use <kbd>video_clipper_gui.py</kbd> to create and save short video clips of specific behaviors into categorized subfolders (e.g., <kbd>clips_for_labeling/Walking/</kbd>).</li>
                        <li><strong>Action 2 (Pose Estimation):</strong> Use the <kbd>yolo_pose_model.pt</kbd> from Phase 1 to run inference on the **newly created video clips**. This will generate corresponding <kbd>.txt</kbd> pose data files.</li>
                        <li><strong>Action 3 (Manual Organization):</strong> Manually move the generated <kbd>.txt</kbd> files into the correct behavior subfolders alongside their corresponding video clips.</li>
                        <li><strong>Action 4 (Consolidation):</strong> Run <kbd>prepare_dataset.py</kbd> to scan these organized folders and create the final training dataset.</li>
                        <li><strong>Output:</strong> A unified folder of labeled pose data ready for training.</li>
                    </ul>
                </div>

                <div class="workflow-arrow">‚¨áÔ∏è</div>

                <div class="p-4 border rounded-lg bg-slate-50">
                    <h4 class="font-bold text-lg text-slate-800">Phase 3: LSTM Classifier Training (This Project)</h4>
                    <ul class="list-disc list-inside mt-2 text-slate-700">
                        <li><strong>Input:</strong> The labeled pose dataset from Phase 2.</li>
                        <li><strong>Action:</strong> Configure and run the <kbd>run_training.py</kbd> script.</li>
                        <li><strong>Output:</strong> A trained LSTM classifier (<kbd>lstm_classifier.pt</kbd>) and normalization statistics (<kbd>norm_stats.pkl</kbd>).</li>
                    </ul>
                </div>

                <div class="workflow-arrow">‚¨áÔ∏è</div>
                
                <div class="p-4 border rounded-lg bg-slate-50">
                    <h4 class="font-bold text-lg text-slate-800">Phase 4: Real-Time Deployment (This Project)</h4>
                    <ul class="list-disc list-inside mt-2 text-slate-700">
                        <li><strong>Input:</strong> Both the <kbd>yolo_pose_model.pt</kbd> (from Phase 1) and the <kbd>lstm_classifier.pt</kbd> (from Phase 3).</li>
                        <li><strong>Action:</strong> Run the <kbd>real_time_inference.py</kbd> script on a new video.</li>
                        <li><strong>Output:</strong> Live, frame-by-frame behavioral classification.</li>
                    </ul>
                </div>
             </div>
        </section>
        
        <section id="tutorial-steps">
            <h2 class="text-3xl font-bold tracking-tight text-slate-900 sm:text-4xl text-center mb-12">Step-by-Step Guide</h2>
            <div class="section-card p-6 md:p-8">

                <div class="admonition admonition-info">
                    <p class="font-bold">Prerequisite</p>
                    <p>This entire workflow relies on a custom-trained YOLO-pose model. You must first use the main IntegraPose toolkit to train a model that can accurately detect the keypoints of your subject.</p>
                </div>

                <div class="step">
                    <div class="step-number">1</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 1: Create Labeled Video Clips</h4>
                        <p class="text-slate-600 mb-4">Generate a dataset of short video clips, each containing a single, clear example of a behavior, using the Video Clipper GUI.</p>
                        
                        <h5 class="font-semibold text-lg mt-4 mb-2">Configuration (`config.json`)</h5>
                        <div class="code-block mt-2">
"dataset_preparation": {
  "source_directory": "clips_for_labeling",
  "key_mapping": { "w": "Walking", "r": "Rearing", ... }
}
                        </div>
                        <p class="text-slate-600 mt-2">Define your output directory and keyboard shortcuts for fast labeling in the config file.</p>

                        <h5 class="font-semibold text-lg mt-4 mb-2">Running the GUI & Labeling</h5>
                        <div class="code-block mt-2">python video_clipper_gui.py</div>
                        <ol class="list-decimal list-inside space-y-2 text-slate-700 mt-4">
                             <li>Click <kbd>Load Video</kbd> to open a long video you want to label.</li>
                             <li>Find a behavior, then use <kbd>Set Start Frame</kbd> and <kbd>Set End Frame</kbd> to mark its boundaries.</li>
                             <li>Assign a label with a keypress (e.g., <kbd>w</kbd> for "Walking").</li>
                             <li>Click <kbd>Add Clip to Queue</kbd>. You can queue many clips for multiple behaviors.</li>
                             <li>When finished, click <kbd>Extract All Queued Clips</kbd>. The GUI will save the clips into a directory structure like the one below.</li>
                        </ol>
                        <div class="directory-structure my-4">
<pre>üìÅ clips_for_labeling/
    ‚îú‚îÄ üìÅ Walking/
    ‚îÇ  ‚îú‚îÄ your_video_Walking_frames_100_250.mp4
    ‚îÇ  ‚îî‚îÄ ...
    ‚îú‚îÄ üìÅ Rearing/
    ‚îÇ  ‚îú‚îÄ your_video_Rearing_frames_550_700.mp4
    ‚îÇ  ‚îî‚îÄ ...
    ‚îî‚îÄ üìÅ Grooming/
       ‚îî‚îÄ ...</pre>
                        </div>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">2</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 2: Generate and Organize Pose Data</h4>
                        <p class="text-slate-600 mb-4">Now that you have video clips, you must generate the corresponding keypoint data for them.</p>

                        <ol class="list-decimal list-inside space-y-3 text-slate-700">
                            <li><strong>Run Pose Estimation:</strong> Use your trained IntegraPose YOLO model (from Phase 1) to perform inference on the folders containing your new video clips (e.g., run `yolo track model=yolo_pose_model.pt source='clips_for_labeling/Walking/' ...`). This will produce a <kbd>.txt</kbd> file for each frame of each clip.</li>
                            <li><strong>Organize Files:</strong> After inference, you must manually move the generated <kbd>.txt</kbd> files into the correct behavior subfolders, placing them alongside their corresponding video clips.</li>
                        </ol>

                        <div class="admonition admonition-warning mt-4">
                            <p class="font-bold">üö® Critical Manual Step</p>
                            <p>For the next step to work, each behavior subfolder (e.g., <kbd>clips_for_labeling/Walking/</kbd>) must contain **both** the video clip files (<kbd>.mp4</kbd>) and their corresponding pose data files (<kbd>.txt</kbd>).</p>
                        </div>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">3</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 3: Prepare the Dataset</h4>
                        <p class="text-slate-600 mb-4">This script automates the final consolidation of your pose data into a training-ready format.</p>
                        
                        <h5 class="font-semibold text-lg mt-4 mb-2">Configuration and Execution</h5>
                        <p class="text-slate-600 mb-2">Ensure your <kbd>class_mapping</kbd> in <kbd>config.json</kbd> correctly maps behavior names to integer IDs. Then run:</p>
                        <div class="code-block mt-2">python prepare_dataset.py</div>
                        <p class="text-slate-600 mt-2">The script scans the subfolders, finds all <kbd>.txt</kbd> files, replaces their internal class ID with the correct one, and copies them into the final training directory specified by <kbd>data.pose_directory</kbd>.</p>
                    </div>
                </div>

                <div class="step">
                    <div class="step-number">4</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 4: Train the Classifier</h4>
                        <p class="text-slate-600 mb-4">This stage trains the LSTM model on your prepared dataset.</p>
                        
                        <h5 class="font-semibold text-lg mt-4 mb-2">Understanding the Sequence Length</h5>
                        <p class="text-slate-600 mb-2">A critical parameter is the <kbd>sequence_length</kbd>, which defines the LSTM's "rolling buffer" or "memory".</p>
                        <div class="code-block mt-2">
"sequence_params": {
  "sequence_length": 15
}
                        </div>
                        <p class="text-slate-600 mt-2">This value in <kbd>config.json</kbd> dictates how many consecutive frames the LSTM analyzes to make a single behavioral prediction. A shorter length (e.g., 15 frames at 30fps = 0.5s) is good for quick behaviors, while a longer length may better capture complex sequences but will have a greater delay in classification.</p>

                        <h5 class="font-semibold text-lg mt-4 mb-2">Running the Training</h5>
                        <div class="code-block mt-2">python run_training.py</div>
                        <p class="text-slate-600 mt-2">The script will compute features, create sequences of the specified length, and train the model. It uses SMOTE and weighted loss to handle imbalanced data. Review the final **Classification Report** in the terminal to judge model performance. This saves the <kbd>lstm_classifier.pt</kbd> (the model) and <kbd>norm_stats.pkl</kbd> (normalization data) to your output folder.</p>
                    </div>
                </div>

                 <div class="step">
                    <div class="step-number">5</div>
                    <div>
                        <h4 class="font-semibold text-xl mb-2">Stage 5: Run Real-Time Inference</h4>
                        <p class="text-slate-600 mb-4">Deploy your models for live classification.</p>
                        
                        <h5 class="font-semibold text-lg mt-4 mb-2">The Rolling Buffer in Action</h5>
                        <p class="text-slate-600 mb-2">The inference script maintains a buffer of poses for each tracked animal, with a maximum size equal to your <kbd>sequence_length</kbd>. It only makes a prediction when this buffer is full. As new frames arrive, the oldest pose is dropped and the newest is added, creating a "rolling window" of temporal analysis that continuously feeds the LSTM.</p>

                        <h5 class="font-semibold text-lg mt-4 mb-2">Execution</h5>
                        <p class="text-slate-600 mb-2">Ensure your trained YOLO model, LSTM model, and norm_stats file are in the correct locations. Update <kbd>config.json</kbd> to point to the video you wish to analyze. Then run:</p>
                        <div class="code-block mt-2">python real_time_inference.py</div>
                        <p class="text-slate-600 mt-2">Press <kbd>q</kbd> to quit and <kbd>t</kbd> to toggle the YOLO skeleton overlay on and off.</p>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <footer class="border-t border-slate-200 mt-12 py-8">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center text-slate-500">
            <p>IntegraPose Real-Time LSTM Classification Guide</p>
        </div>
    </footer>

</body>
</html>