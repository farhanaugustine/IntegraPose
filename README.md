[![DOI](https://zenodo.org/badge/988759361.svg)](https://doi.org/10.5281/zenodo.15565090)
* Preprint availabe: [â¡ï¸ğŸ“‘](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5334465)

# IntegraPose - Simultaneous Behavior Classification and Pose Estimation
<div align="center">
<img src="https://github.com/user-attachments/assets/2aec1a90-552d-42e6-8ae4-91ae322a6f1b" alt="IntegraPose" width="50%"/>
</div>

This repository demonstrates multi-task learning for combined object detection and pose-estimation using YOLO-Pose models. IntegraPose enables robust, simultaneous classification of complex behaviors and precise keypoint tracking from a single model. IntegraPose is a comprehensive suite of tools designed to streamline the entire workflow of behavioral analysis, from initial video annotation to advanced, post-hoc analysis of model outputs. It consists of a primary GUI-based application that integrates various functionalities into a single, cohesive workflow.

# ğŸš§ Project Status ğŸš§
This repository is currently under active development. The tools are functional, but APIs may change, and new features will be added. Feedback and contributions are welcome!

# âœ¨ Key Features

1. Simultaneous Tracking & Classification: A single model predicts bounding boxes, keypoint locations, and behavioral class.
2. Modular GUI-Driven Workflow: Easy-to-use graphical interfaces for annotation, training, and analysis, minimizing the need for complex command-line interaction.
3. Integrated Annotation Tool: Interactively label keypoints and assign behavioral classes to subjects in your images.
4. Advanced Bout Analytics: Tools for analyzing, confirming, and scoring behavioral bouts, including manual review and correction.
5. Pose Clustering Analysis: A dedicated tab for exploratory data analysis, clustering, and visualization of pose data.
6. Flexible ROI Management: Define regions of interest dynamically for various analysis modes.

# ğŸ› ï¸ Prerequisites
Before you begin, ensure you have the following installed:

* Python: Version > 3.11 (tested with 3.11 & 3.12).
* Conda: Recommended for managing dependencies.
Core Libraries (can be installed via pip after setting up a Conda environment):
```
pip install ultralytics opencv-python pandas numpy hdbscan umap-learn matplotlib pyyaml scipy scikit-learn openpyxl XlsxWriter
```
* Alternatively, you can create a Conda environment using the provided `environment.yml` file:
```
conda env create -f environment.yml
conda activate IntegraPose
```

# ğŸš€ Workflow & How to Use
The IntegraPose workflow is managed through a single main GUI application.

To get started, run `main_gui_app.py` located in BehaviorAnnotation_ModelTraining Folder:

```
python main_gui_app.py
```

The application is organized into several tabs, each corresponding to a stage or type of analysis:

## 1. Setup & Annotation Tab
This tab is for defining your project's keypoints and behaviors, and for launching the integrated annotation tool.
* Define Keypoint Names: Specify the ordered names of your keypoints (e.g., nose, left_ear, right_ear).
* Define Behaviors: List the behaviors you want to classify, each with a unique numeric ID.
* Configure Annotation Directories: Set paths for your source images and where the generated annotation .txt files will be saved.
* Launch Annotator: Click "Launch Annotator" to open the OpenCV-based interactive tool for drawing bounding boxes and labeling keypoints. The annotator supports zoom, pan, and autosave.
* Generate dataset.yaml: After annotation, use this section to create the YOLO-compatible dataset.yaml file, which is crucial for model training.
## 2. Model Training Tab
Once your dataset is ready, use this tab to configure and initiate the training of your YOLO-Pose model.
* Load Dataset YAML: Point to the dataset.yaml file generated in the Setup tab.
* Select Model Variant: Choose a pre-trained YOLO-Pose model (e.g., yolov8n-pose.pt).
* Adjust Hyperparameters: Configure epochs, learning rate, batch size, and other training parameters.
* Data Augmentation Settings: Control various augmentation techniques like flip, perspective, and mosaic.
* Start Training: Launch the Ultralytics training process directly from the GUI.
## 3. Inference Tab
Use this tab to run your trained model on new video files or folders containing images.
* Load Trained Model: Select your `.pt` model file.
* Select Source: Choose a video file or a folder of images for inference.
* Configure Parameters: Adjust confidence thresholds, IOU, and other inference settings.
* Output Options: Define where output videos and detection .txt files will be saved.
* Start Inference: Process your data and generate detection outputs.
## 4. Webcam Inference Tab
Perform real-time inference using your trained model with a live webcam feed.
* Load Trained Model: Select your .pt model.
* Webcam Index: Specify the index of your webcam (e.g., 0 for the default).
* Live View & Recording: View the live inference and optionally record the annotated video.
## 5. Bout Analytics Tab
This tab provides tools for analyzing and managing behavioral bouts from your YOLO inference output.
* Load YOLO Output: Load your YOLO detection .txt files (from a folder or individual files).
* ROI Management:
   * Draw New ROI: Interactively draw regions of interest on a video frame. The tool guides you through drawing polygons.
   * Load/Save ROIs: Import and export ROI definitions to/from YAML files for reusability.
<img width="751" alt="ROI_Drawing" src="https://github.com/user-attachments/assets/5da6beba-bee1-47cd-8bf3-3ee5044099ca" />

* Behavioral Bout Analysis:
   * Calculate bout statistics (duration, frequency, time between bouts).
   * Filter and analyze bouts based on ROI entry/exit.
   * Review & Confirm Detected Bouts: Launch a dedicated tool to manually review and confirm automatically detected bouts.
   *  This tool allows for corrections and detailed scoring, saving updated bout information to a CSV file.
   <img width="976" alt="bout_confirmation_tool_v1" src="https://github.com/user-attachments/assets/98191ae4-4db2-48a4-a5ca-2638d305dbd2" />

   * Open Advanced Bout Scorer {**Work in-progress**}: Access an advanced interface for in-depth manual scoring and verification of behavioral bouts from a video.
## 6. Pose Clustering Analysis Tab
This tab allows for exploratory data analysis, including normalization, feature engineering, and clustering of pose data.
* Load Data: Import pose data (from YOLO output or other sources).
* Define Keypoints & Skeleton: Specify keypoint names and skeleton connections for feature calculations.
* Normalization & Feature Engineering: Normalize pose data and generate features based on keypoints and skeleton definitions.
* Clustering: Apply UMAP and HDBSCAN clustering to discover patterns in your behavioral data.
  ![clusters_video_1_720p_Walking_track1](https://github.com/user-attachments/assets/b566b074-c748-45b4-8c87-deeb46803c22)
  <img width="751" alt="Pose_Clustering" src="https://github.com/user-attachments/assets/c15429fb-d711-4d09-a129-fc24d8b0de14" />


* Visualization: Visualize clusters and their relationship to the original data.
# Project Structure
The repository is organized as follows:

```
BehaviorAnnotation_ModelTraining/
â”œâ”€â”€ main_gui_app.py
â”œâ”€â”€ keypoint_annotator_module.py
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ advanced_bout_analyzer.py
â”‚   â”œâ”€â”€ bout_confirmation_tool.py
â”‚   â”œâ”€â”€ inference_tab.py
â”‚   â”œâ”€â”€ log_tab.py
â”‚   â”œâ”€â”€ pose_clustering_tab.py
â”‚   â”œâ”€â”€ roi_analytics_tab.py
â”‚   â”œâ”€â”€ setup_tab.py
â”‚   â”œâ”€â”€ tooltips.py
â”‚   â”œâ”€â”€ training_tab.py
â”‚   â””â”€â”€ webcam_tab.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ bout_analyzer.py
â”‚   â”œâ”€â”€ command_builder.py
â”‚   â”œâ”€â”€ config_manager.py
â”‚   â”œâ”€â”€ roi_drawing_tool.py
â”‚   â”œâ”€â”€ roi_manager.py
â”‚   â”œâ”€â”€ video_creator.py
â”‚   â””â”€â”€ webcam_manager.py
â”œâ”€â”€ environment.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ bout_tool.log (example log file)
```

# Showcase ğŸ–¼ï¸
Here are some examples of IntegraPose in action, classifying distinct behaviors while simultaneously tracking keypoints.
| OpenField| Video Source | Behaviors |
|---|---|---|
|![Github_BehaviorDepot_1](https://github.com/user-attachments/assets/7946b7f0-7941-4126-b1f4-788b9a7029d8)| Video Source: [BehaviorDEPOT](https://github.com/DeNardoLab/BehaviorDEPOT)| Walking, Wall-Rearing/Supported Rearing|
|![Github_BehaviorDepot_2](https://github.com/user-attachments/assets/acddccf2-7838-4131-98d1-7ef79e1d8a0a)| Video Source: [BehaviorDEPOT](https://github.com/DeNardoLab/BehaviorDEPOT) | Walking, Wall-Rearing/Supported Rearing|
|![Github_BehaviorDepot_3](https://github.com/user-attachments/assets/7cce1887-1eac-439b-bb1a-9252c06df2a5)| Video Source: [BehaviorDEPOT](https://github.com/DeNardoLab/BehaviorDEPOT) | Walking, Grooming|
|![Github_DeerMice_4](https://github.com/user-attachments/assets/1a86d84f-e5c9-4aae-95bf-c17c96b00f13)| Video Source: [Temporal_Behavior_Analysis](https://github.com/farhanaugustine/Temporal_Behavior_Analysis)| Exploring/Waling, Wall-Rearing/Supported Rearing|
|![Github_DeerMice_5](https://github.com/user-attachments/assets/8ef72a1a-08a9-4aba-9378-22519ba2b69d)| Video Source: [Temporal_Behavior_Analysis](https://github.com/farhanaugustine/Temporal_Behavior_Analysis) | Wall-Rearing/Supported Rearing, Jump|
|![Github_BehaviorDepot_6](https://github.com/user-attachments/assets/8e86dec0-6539-4b93-9bce-7d45aebb5353)| Video Source: [BehaviorDEPOT](https://github.com/DeNardoLab/BehaviorDEPOT) | Ambulatory/Walking, Object Exploration, Object Mounting |
|![Github_BehaviorDepot_7](https://github.com/user-attachments/assets/e574b720-ce13-4190-88d5-ef2faceeefee)| Video Source: [BehaviorDEPOT](https://github.com/DeNardoLab/BehaviorDEPOT) | Ambulatory/Walking, Object Exploration |
|![Github_C57B_8](https://github.com/user-attachments/assets/bb9f0491-0d9f-4a97-80f9-6bc059b337d1)| Video Source: Self | Ambulatory/Walking, Nose-Poking, Wall-Rearing/Supported Rear |
|![Github_CHKO_9](https://github.com/user-attachments/assets/17b4d4da-fc77-4dff-a04a-000bbfef96a8)| Video Source: Self | Ambulatory/Walking, Wall-Rearing/Supported Rear |

# Citation Suggestions:
Augustine, et al., (2025). Integrapose: A Unified Framework for Simultaneous Pose Estimation and Behavior Classification. Availabe on SSRN: https://ssrn.com/abstract=5334465

Repo Underdevelopment ğŸš§ğŸ—ï¸ğŸ‘·ğŸ¼




